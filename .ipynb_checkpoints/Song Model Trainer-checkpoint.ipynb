{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is adapted from the fastai IMDb example at:\n",
    "# https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb\n",
    "from fastai_old.text import *\n",
    "import html\n",
    "import spacy \n",
    "\n",
    "spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Here is where the learner for training is initialized. The first cell implements the scraping of lyrics from Genius.com and the text pre-processing for use later on. The scraper takes an array of artist names, searches for those artists, and for the artists that the user confirms it downloads their entire discography. Then pre_process() separates and sorts the song components into respective folders in the \"data\" directory. After that all the component files are loaded and further preprocessed to into word elements.\n",
    "\n",
    "Before using this model the 'models.tar.gz' file must be downloaded from:\n",
    "\n",
    "https://github.com/peterspenler/Modelling-Complex-Lyric-Project/releases/tag/v1.0\n",
    "\n",
    "and extracted into the 'data' folder in the root project directory. This archive contains the trained models necessary for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the Genius scraping and pre-processing libraries we created\n",
    "from scraper import get_lyrics\n",
    "from concatenate_data import pre_proccess\n",
    "\n",
    "artists = [\"Kanye\", \"Jay-Z\", \"2pac\"] #This is an array of artists to scrape\n",
    "get_lyrics(artists) #This gets the lyrics and saves them in \"test_data\"\n",
    "pre_proccess() #This splits the components from each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=Path('data/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "#! curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "#! tar -xzfv aclImdb_v1.tar.gz -C {DATA_PATH}\n",
    "\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_PATH=Path('data/model_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['unsup']\n",
    "maxn = 60000\n",
    "component = 'verse' #This is the component of the song to train\n",
    "\n",
    "def get_texts(path):\n",
    "    texts = []\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "        for fname in (path).glob('*.*'):\n",
    "            if len(texts) >= maxn:\n",
    "                break\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "    return np.array(texts)\n",
    "\n",
    "all_texts = get_texts(PATH/component)\n",
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts,val_texts = sklearn.model_selection.train_test_split(\n",
    "    all_texts, test_size=0.1)\n",
    "\n",
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['text']\n",
    "df_trn = pd.DataFrame({'text':trn_texts}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts}, columns=col_names)\n",
    "\n",
    "df_trn['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df):\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df['text'].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = get_texts(df_trn)\n",
    "tok_val = get_texts(df_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_PATH_TMP=Path('data/model_lm/tmp/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH_TMP/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH_TMP/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where and integer mapping is made fore each word unit. These integers are what the model is then trained on. A string mapping is also made to convert the outputted integers back into their respective words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only pick words that are used more than once to omit obscure words which then can't be trained well\n",
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=len(itos)\n",
    "vs,len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the pre-trained wt103 model is loaded to act as the base for the new model we're going to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "PRE_PATH = PosixPath('data/models')\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "\n",
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "len(itos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = set(itos) - set(itos2).intersection(itos)\n",
    "len(oov), list(oov)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are loading in the pre-trained weights for the words in our lyric corpus, which also appear in the wt103 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):                     # for word in lyrics vocab\n",
    "    r = stoi2[w]                                # get the int in the pretrained vocab\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m   # add weight if in vocab, else add mean weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we actually create the learner we will be training with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=32\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two funcitons for generating text to test the progress of our model. generate_text2() always picks the next word from a distribution, but generate_text3() sometimes picks the top result to try and increase the uniformity of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text2(m, s, l=20):\n",
    "    m[0].bs=1  # Set batch size to 1\n",
    "    m.eval()  # Turn off dropout\n",
    "    m.reset()  # Reset hidden state\n",
    "    m[0].bs=bs  # Put the batch size back to what it was\n",
    "\n",
    "    ss = s.lower().split()\n",
    "    si = [stoi[w] for w in ss]\n",
    "    t = torch.autograd.Variable(torch.cuda.LongTensor(np.array([si])))\n",
    "    \n",
    "    res,*_ = m(t)\n",
    "\n",
    "    print(s, end=' ')\n",
    "    for i in range(l):\n",
    "        #n = res[-1].topk(5)[1]  # top word\n",
    "        n = torch.multinomial(res[-1].exp(), 3)  # drawing from probability distribution\n",
    "        n = n[1] if n.data[0]==0 else n[0]\n",
    "        print(itos2[int(n)], end=' ')\n",
    "        res,*_ = m(n.unsqueeze(0).unsqueeze(0))  # sometimes need an extra .unsqueeze(0)\n",
    "    print('...')\n",
    "    \n",
    "def generate_text3(m, s, l=20):\n",
    "    m[0].bs=1  # Set batch size to 1\n",
    "    m.eval()  # Turn off dropout\n",
    "    m.reset()  # Reset hidden state\n",
    "    m[0].bs=bs  # Put the batch size back to what it was\n",
    "\n",
    "    ss = s.lower().split()\n",
    "    si = [stoi[w] for w in ss]\n",
    "    t = torch.autograd.Variable(torch.cuda.LongTensor(np.array([si])))\n",
    "    \n",
    "    res,*_ = m(t)\n",
    "    \n",
    "    print(s, end=' ')\n",
    "    count = 0;\n",
    "    while True:\n",
    "        if np.random.choice([0,1], p=[0.05,0.95]) == 0:\n",
    "            n = res[-1].topk(5)[1]  # top word\n",
    "        else:\n",
    "            n = torch.multinomial(res[-1].exp(), 10)  # drawing from probability distribution\n",
    "        n = n[1] if n.data[0]==0 else n[0]\n",
    "        if itos[int(n)] == '\\n' and count > l:\n",
    "            print('')\n",
    "            break\n",
    "        if not any (x in itos[int(n)] for x in ['xbos', 'xfld']):\n",
    "            print(itos[int(n)], end=' ')\n",
    "        res,*_ = m(n.unsqueeze(0).unsqueeze(0))  # sometimes need an extra .unsqueeze(0)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text2(m, \"The\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Here we first start with a few training epochs to see a preliminary result of training our lyric data onto the pretrained model. We first use learner.find_lr() to try and find an ideal learning rate, and then use learner.fit() to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_song = itos\n",
    "with open(PRE_PATH/'itos_song.pkl', 'wb') as f:\n",
    "    pickle.dump(itos_song, f)\n",
    "learner.save('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text2(m, \"the\", l=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train for another 10 epochs to see how much the model can improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_3epochs')\n",
    "m=learner.model\n",
    "generate_text2(m, \"the way it is\", l=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen a bit of improvement we're going to go for the real deal and train the model for 30 epochs. This will give us a final model that should produce lyrics that are clearly within the style of our genre. Performing this step with 30 epochs can start to overfit the model if the training set is not large enough so num_epochs should be adjusted depending on the training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=num_epochs)\n",
    "learner.save('lm_30epochs-verse-country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=learner.model\n",
    "generate_text3(m, \"this day they play\", l=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we ensure that the model and word mapping are saved so that we can use them for generation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_30epochs-new-model')\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos-new-model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
